{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# armazenar os documentos totais em variavel\n",
    "\n",
    "import os\n",
    "rootdir = '/media/r2-d2/E0C494CDC494A6F6/doutorado/20_newsgroup'\n",
    "#rootdir = '/media/developer/DATA/doutorado/rec informacao/20_newsgroup'\n",
    "docs = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #if os.path.basename(subdir) in ['alt.atheism', 'rec.sport.baseball']:\n",
    "        with open(os.path.join(subdir, file), encoding='cp1252') as f:                \n",
    "            contents = f.read()\n",
    "            docs.append({ 'tema': os.path.basename(subdir), 'nome_arquivo': file, 'conteudo': contents  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/r2-d2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/r2-d2/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# baixando as libs para pre processamento\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#stemmer = PorterStemmer()\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_header(text):\n",
    "    # Find the first occurrence of '\\n\\n'\n",
    "    pos = text.find('\\n\\n')\n",
    "\n",
    "    # Remove everything after the first '\\n\\n'\n",
    "    if pos >= 0:\n",
    "        text = text[pos+2:]\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_email(text):\n",
    "    pattern = r'\\S+@\\S+'\n",
    "    # Remove email addresses from the string\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre processamento\n",
    "\n",
    "    # Remover caracteres especiais e números\n",
    "    #text = re.sub('[^A-Za-z\\s\\']+', '', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "\n",
    "    #text = remove_header(text)\n",
    "\n",
    "    #text = remove_email(text)\n",
    "\n",
    "\n",
    "    # Converter tudo para letras minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover as tags HTML\n",
    "    text = re.sub('<[^<]+?>', '', text)\n",
    "    \n",
    "    # manter so texto\n",
    "    #text = re.sub('[^A-Za-z ]+', '', text)\n",
    "\n",
    "    \n",
    "    # Tokenizar o texto em palavras\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # remover palavras que so possua caractere especial\n",
    "    words = [word for word in words if not re.match('^[^A-Za-z0-9]+$', word)]\n",
    "\n",
    "    # remover palavras que tenha tamanho 1\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "\n",
    "    \n",
    "    # Remover as stopwords\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    # Aplicar o stemming\n",
    "    #words = [stemmer.stem(word) for word in words]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Juntar as palavras em uma única string\n",
    "    text = ' '.join(words)\n",
    "  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rodar o pre processamento\n",
    "\n",
    "def preprocess_collection(docs):\n",
    "\n",
    "    preprocessed_documents = []\n",
    "\n",
    "    for document in docs:\n",
    "        tema = document['tema']\n",
    "        nome_arquivo = document['nome_arquivo']\n",
    "        conteudo = document['conteudo']\n",
    "        \n",
    "        preprocessed_conteudo = preprocess_text(conteudo)\n",
    "        \n",
    "        preprocessed_document = {\n",
    "            'tema': tema,\n",
    "            'nome_arquivo': nome_arquivo,\n",
    "            'conteudo': preprocessed_conteudo\n",
    "        }\n",
    "        \n",
    "        preprocessed_documents.append(preprocessed_document)\n",
    "        \n",
    "    return preprocessed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_documents = preprocess_collection(docs)\n",
    "frases = [o['conteudo'] for o in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xref cantaloupe.srv.cs.cmu.edu talk.abortion:120737 alt.atheism:53341 talk.religion.misc:83726 path cantaloupe.srv.cs.cmu.edu magnesium.club.cc.cmu.edu news.sei.cmu.edu cis.ohio-state.edu pacific.mps.ohio-state.edu zaphod.mps.ohio-state.edu usc sol.ctr.columbia.edu ira.uka.de germany.eu.net thoth.mchp.sni.de horus.ap.mchp.sni.de d012s658 frank frank d012s658.uucp frank o'dwyer newsgroups talk.abortion alt.atheism talk.religion.misc subject 2000 year say christian morality date apr 1993 20:00:08 gmt organization siemens-nixdorf line message-id reference nntp-posting-host d012s658.ap.mchp.sni.de article mathew writes frank d012s658.uucp frank o'dwyer writes article mathew mantis.co.uk mathew writes ask think wrong relativism correct misconception well cut chase admit find least attractive realtivism elevates heinous level good saying effect good n't good thus terrorist elevated level man peace complete nonsense relativism mean saying absolute standard morality mean saying standard morality equally good presumably mean moral system better others manage without objective frame reference weasel word use describe frame reference n't objective reality value frank o'dwyer hatching that' odwyer sse.ie hen evelyn conlon\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in frases]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# armazenar os documentos totais em variavel\n",
    "\n",
    "import os\n",
    "rootdir = '/media/r2-d2/E0C494CDC494A6F6/doutorado/mini_newsgroups'\n",
    "docs_consultas = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        with open(os.path.join(subdir, file), encoding='cp1252') as f:                \n",
    "            contents = f.read()\n",
    "            docs_consultas.append({ 'tema': os.path.basename(subdir), 'nome_arquivo': file, 'conteudo': contents  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mini_newsgroups': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'alt.atheism': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'comp.graphics': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'comp.os.ms-windows.misc': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'comp.sys.ibm.pc.hardware': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'comp.sys.mac.hardware': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'comp.windows.x': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'misc.forsale': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'rec.autos': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'rec.motorcycles': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'rec.sport.baseball': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'rec.sport.hockey': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'sci.crypt': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'sci.electronics': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'sci.med': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'sci.space': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'soc.religion.christian': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'talk.politics.guns': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'talk.politics.mideast': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'talk.politics.misc': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}, 'talk.religion.misc': {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}}\n"
     ]
    }
   ],
   "source": [
    "dirs_abs = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    if os.path.basename(subdir) != '20_newsgroup':\n",
    "        dirs_abs.append(os.path.basename(subdir))\n",
    "\n",
    "temas = {}\n",
    "for d in dirs_abs:\n",
    "    temas[d] = {'p10': 0, 'p20': 0, 'p50': 0, 'p100': 0, 'map': 0}\n",
    "\n",
    "print(temas)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_map(ranking, tema):\n",
    "    acertos = 0\n",
    "    maps = []\n",
    "    for i, doc in enumerate(ranking):\n",
    "        if tema in doc:\n",
    "            acertos +=1\n",
    "            maps.append(acertos/(i+1))\n",
    "    if len(maps) > 0:\n",
    "        return mean(maps)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_consultas_preproc = preprocess_collection(docs_consultas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"newsgroups alt.atheism path cantaloupe.srv.cs.cmu.edu das-news.harvard.edu noc.near.net howland.reston.ans.net zaphod.mps.ohio-state.edu darwin.sura.net haven.umd.edu uunet mnemosyne.cs.du.edu nyx jcopelan jcopelan nyx.cs.du.edu one subject message-id sender usenet mnemosyne.cs.du.edu netnews admin account organization salvation army draft board reference date fri apr 13:38:30 gmt line article geoff east.sun.com writes posting provoked checking save file memorable post first captured ken arromdee feb 1990 subject atheist article 473 question article 53766 average article day last three year others noted current posting rate kill file depressing large among posting saved early day article following notable loren sunlight.llnl.gov loren petrich jchrist nazareth.israel.rel jesus christ nazareth mrc tomobiki-cho.cac.washington.edu mark crispin perry apollo.hp.com jim perry lippard uavax0.ccit.arizona.edu james lippard minsky media.mit.edu marvin minsky interesting bunch wonder n't hear address changed reached following address dkoresh branch.davidian.compound.waco.tx.us think last seen posting alt.messianic jim god dead actor play part sting word fear find way place heart history without voice reason every faith curse teach without freedom past thing get worse nothing\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_consultas_preproc[0]['conteudo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_query = doc_consultas_preproc[0]['conteudo'].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25.get_top_n(tokenized_query, frases, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "ename": "StatisticsError",
     "evalue": "mean requires at least one data point",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb Cell 17\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ranking \u001b[39m=\u001b[39m bm25\u001b[39m.\u001b[39mget_top_n(tokenized_query, frases, n\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m acertos \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m map_f\u001b[39m.\u001b[39mappend(calcula_map(ranking, doc_consulta[\u001b[39m'\u001b[39;49m\u001b[39mtema\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, doc \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ranking):\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m#print(f\"Documento {doc[0]} - Target {doc[1]} -->  {doc[2]} - Similaridade: {doc[3]}\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m#if doc[1] == doc_consulta['tema']:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mif\u001b[39;00m doc_consulta[\u001b[39m'\u001b[39m\u001b[39mtema\u001b[39m\u001b[39m'\u001b[39m] \u001b[39min\u001b[39;00m doc:\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb#X26sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39m#print(f\"{doc_consulta['tema']} e {doc}\")\u001b[39;00m\n",
      "\u001b[1;32m/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb Cell 17\u001b[0m in \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         acertos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         maps\u001b[39m.\u001b[39mappend(acertos\u001b[39m/\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/r2-d2/E0C494CDC494A6F6/doutorado/rec_info/bm25.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mreturn\u001b[39;00m mean(maps)\n",
      "File \u001b[0;32m/usr/lib/python3.10/statistics.py:328\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    326\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mraise\u001b[39;00m StatisticsError(\u001b[39m'\u001b[39m\u001b[39mmean requires at least one data point\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    329\u001b[0m T, total, count \u001b[39m=\u001b[39m _sum(data)\n\u001b[1;32m    330\u001b[0m \u001b[39massert\u001b[39;00m count \u001b[39m==\u001b[39m n\n",
      "\u001b[0;31mStatisticsError\u001b[0m: mean requires at least one data point"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# consultando\n",
    "p10 = []\n",
    "p20 = []\n",
    "p50 = []\n",
    "p100 = []\n",
    "map_f = []\n",
    "tema_atual = \"\"\n",
    "\n",
    "for c, doc_consulta in enumerate(doc_consultas_preproc):\n",
    "    clear_output(wait=True)\n",
    "    print(c)\n",
    "    #ranking = similaridade(doc_consulta['conteudo'], preprocessed_documents, idfs)\n",
    "    ranking = bm25.get_top_n(tokenized_query, frases, n=100)\n",
    "    acertos = 0\n",
    "    \n",
    "    map_f.append(calcula_map(ranking, doc_consulta['tema']))\n",
    "    \n",
    "    for i, doc in enumerate(ranking):\n",
    "        #print(f\"Documento {doc[0]} - Target {doc[1]} -->  {doc[2]} - Similaridade: {doc[3]}\")\n",
    "        #if doc[1] == doc_consulta['tema']:\n",
    "        \n",
    "        if doc_consulta['tema'] in doc:\n",
    "            #print(f\"{doc_consulta['tema']} e {doc}\")\n",
    "            acertos+=1\n",
    "        if i+1 == 10:\n",
    "            p10.append(acertos/10.0)            \n",
    "        if i+1 == 20:\n",
    "            p20.append(acertos/20.0)\n",
    "        if i+1 == 50:\n",
    "            p50.append(acertos/50.0)\n",
    "        if i+1 == 100:\n",
    "            p100.append(acertos/100.0) \n",
    "        #map.append(calcula_map(ranking, doc_consulta['tema']))\n",
    "    if c > 0:\n",
    "        if tema_atual != doc_consulta['tema'] or c == len(doc_consultas_preproc) - 1:\n",
    "            temas[tema_atual]['p10'] = mean(p10)\n",
    "            temas[tema_atual]['p20'] = mean(p20)\n",
    "            temas[tema_atual]['p50'] = mean(p50)\n",
    "            temas[tema_atual]['p100'] = mean(p100)\n",
    "            temas[tema_atual]['map'] = mean(map_f)\n",
    "            p10 = []\n",
    "            p20 = []\n",
    "            p50 = []\n",
    "            p100 = []\n",
    "            map = []\n",
    "\n",
    "    tema_atual = doc_consulta['tema']\n",
    "    # if c > 102:\n",
    "    #      break    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data_lemma_bm25.json', 'w') as f:\n",
    "    json.dump(temas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(temas)\n",
    "df =df.transpose()\n",
    "df.drop('mini_newsgroups')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
